{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = './data/train.csv'\n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = './data/test.csv'\n",
    "_, x_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of 's' label in y is 34.27%\n",
      "The percentage of 'b' label in y is 65.73%\n"
     ]
    }
   ],
   "source": [
    "# check the percentage of two labels\n",
    "print(\"The percentage of 's' label in y is {:.2f}%\".format(100 * y[y == 1].size / y.size))\n",
    "print(\"The percentage of 'b' label in y is {:.2f}%\".format(100 * y[y == -1].size / y.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data into training and validation\n",
    "y_train, y_val, x_train, x_val = train_val_split(y, x, 0.2, seed=1)\n",
    "\n",
    "# normalize data using metrics of training data (except PRI_jet_num (22th column) since it is a discrete value)\n",
    "nor_indices = [idx for idx in range(x_train.shape[1]) if idx != 22]\n",
    "nor_x_train = x_train.copy()\n",
    "nor_x_val = x_val.copy()\n",
    "nor_x_test = x_test.copy()\n",
    "\n",
    "x_train_mean = x_train[:, nor_indices].mean(axis=0)\n",
    "x_train_std = x_train[:, nor_indices].std(axis=0)\n",
    "\n",
    "nor_x_train[:, nor_indices] = (nor_x_train[:, nor_indices] - x_train_mean) / x_train_std\n",
    "nor_x_val[:, nor_indices] = (nor_x_val[:, nor_indices] - x_train_mean) / x_train_std\n",
    "nor_x_test[:, nor_indices] = (nor_x_test[:, nor_indices] - x_train_mean) / x_train_std\n",
    "\n",
    "# add all ones column to features for bias term\n",
    "nor_x_train = np.c_[np.ones((nor_x_train.shape[0], 1)), nor_x_train]\n",
    "nor_x_val = np.c_[np.ones((nor_x_val.shape[0], 1)), nor_x_val]\n",
    "nor_x_test = np.c_[np.ones((nor_x_test.shape[0], 1)), nor_x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression using gradient descent\n",
      "Gamma: 0.175\n",
      "Training Loss: 64032330817782258699348455100848597920567209034474802636499185850267362538230598828223933528095207305235637763000162582528.0000 - Training Accuracy: 0.3736\n",
      "Validation Loss: 84521039643137521684413594552378521643441140801189601383227715639707775688040518795695187604554802351077332321329154621440.0000 - Validation Accuracy: 0.3731\n",
      "Gamma: 0.15\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7446\n",
      "Validation Loss: 0.3400 - Validation Accuracy: 0.7441\n",
      "Gamma: 0.1\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7446\n",
      "Validation Loss: 0.3400 - Validation Accuracy: 0.7440\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "max_iters = 1000\n",
    "initial_w = np.zeros(nor_x_train.shape[1])\n",
    "gammas = [0.175, 0.15, 0.1]\n",
    "\n",
    "# create history of weights and validation loss\n",
    "weights_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "print('Linear regression using gradient descent')\n",
    "for gamma in gammas:\n",
    "    # train model, get weights and loss\n",
    "    weights, train_loss = least_squares_GD(y_train, nor_x_train, initial_w, max_iters, gamma)\n",
    "    val_loss = compute_ls_loss(y_val, nor_x_val, weights)\n",
    "\n",
    "    # make prediction\n",
    "    y_train_pred = predict_labels(weights, nor_x_train)\n",
    "    y_val_pred = predict_labels(weights, nor_x_val)\n",
    "\n",
    "    # compute accuracy\n",
    "    train_acc = compute_accuracy(y_train, y_train_pred)\n",
    "    val_acc = compute_accuracy(y_val, y_val_pred)\n",
    "    \n",
    "    # store weights and validation loss\n",
    "    weights_history.append(weights)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print('Gamma:', gamma)\n",
    "    print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "    print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma: 0.15\n"
     ]
    }
   ],
   "source": [
    "# get the best parameters\n",
    "best_idx = np.argmin(val_loss_history)\n",
    "best_gamma = gammas[best_idx]\n",
    "weights = weights_history[best_idx]\n",
    "print('Best gamma:', best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_pred = predict_labels(weights, nor_x_test)\n",
    "OUTPUT_PATH = 'lr_gd.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression using stochastic gradient descent\n",
      "Gamma: 0.01\n",
      "Training Loss: 0.0718 - Training Accuracy: 0.6967\n",
      "Validation Loss: 0.3922 - Validation Accuracy: 0.6987\n",
      "Gamma: 0.005\n",
      "Training Loss: 1.4715 - Training Accuracy: 0.7132\n",
      "Validation Loss: 0.3632 - Validation Accuracy: 0.7144\n",
      "Gamma: 0.001\n",
      "Training Loss: 0.0663 - Training Accuracy: 0.7166\n",
      "Validation Loss: 0.3659 - Validation Accuracy: 0.7171\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "max_iters = 1000\n",
    "initial_w = np.zeros(nor_x_train.shape[1])\n",
    "gammas = [0.01, 0.005, 0.001]\n",
    "\n",
    "# create history of weights and validation loss\n",
    "weights_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "print('Linear regression using stochastic gradient descent')\n",
    "for gamma in gammas:\n",
    "    # train model, get weights and loss\n",
    "    weights, train_loss = least_squares_SGD(y_train, nor_x_train, initial_w, max_iters, gamma)\n",
    "    val_loss = compute_ls_loss(y_val, nor_x_val, weights)\n",
    "\n",
    "    # make prediction\n",
    "    y_train_pred = predict_labels(weights, nor_x_train)\n",
    "    y_val_pred = predict_labels(weights, nor_x_val)\n",
    "\n",
    "    # compute accuracy\n",
    "    train_acc = compute_accuracy(y_train, y_train_pred)\n",
    "    val_acc = compute_accuracy(y_val, y_val_pred)\n",
    "    \n",
    "    # store weights and validation loss\n",
    "    weights_history.append(weights)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print('Gamma:', gamma)\n",
    "    print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "    print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma: 0.005\n"
     ]
    }
   ],
   "source": [
    "# get the best parameters\n",
    "best_idx = np.argmin(val_loss_history)\n",
    "best_gamma = gammas[best_idx]\n",
    "weights = weights_history[best_idx]\n",
    "print('Best gamma:', best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_pred = predict_labels(weights, nor_x_test)\n",
    "OUTPUT_PATH = 'lr_sgd.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least squares regression using normal equations\n",
      "Training Loss: 0.3395 - Training Accuracy: 0.7452\n",
      "Validation Loss: 0.3394 - Validation Accuracy: 0.7445\n"
     ]
    }
   ],
   "source": [
    "# train model, get weights and loss\n",
    "weights, train_loss = least_squares(y_train, nor_x_train)\n",
    "val_loss = compute_ls_loss(y_val, nor_x_val, weights)\n",
    "\n",
    "# make prediction\n",
    "y_train_pred = predict_labels(weights, nor_x_train)\n",
    "y_val_pred = predict_labels(weights, nor_x_val)\n",
    "\n",
    "# compute accuracy\n",
    "train_acc = compute_accuracy(y_train, y_train_pred)\n",
    "val_acc = compute_accuracy(y_val, y_val_pred)\n",
    "\n",
    "print('Least squares regression using normal equations')\n",
    "print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_pred = predict_labels(weights, nor_x_test)\n",
    "OUTPUT_PATH = 'ls.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression using normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression using normal equations\n",
      "Lambda: 0.001\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7445\n",
      "Validation Loss: 0.3400 - Validation Accuracy: 0.7436\n",
      "Lambda: 0.0005\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7447\n",
      "Validation Loss: 0.3400 - Validation Accuracy: 0.7438\n",
      "Lambda: 0.0001\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7447\n",
      "Validation Loss: 0.3400 - Validation Accuracy: 0.7439\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "lambdas = [0.001, 0.0005, 0.0001]\n",
    "\n",
    "# create history of weights and validation loss\n",
    "weights_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "print('Ridge regression using normal equations')\n",
    "for lambda_ in lambdas:\n",
    "    # train model, get weights and loss\n",
    "    weights, train_loss = ridge_regression(y_train, nor_x_train, lambda_)\n",
    "    val_loss = compute_ls_loss(y_val, nor_x_val, weights)\n",
    "\n",
    "    # make prediction\n",
    "    y_train_pred = predict_labels(weights, nor_x_train)\n",
    "    y_val_pred = predict_labels(weights, nor_x_val)\n",
    "\n",
    "    # compute accuracy\n",
    "    train_acc = compute_accuracy(y_train, y_train_pred)\n",
    "    val_acc = compute_accuracy(y_val, y_val_pred)\n",
    "    \n",
    "    # store weights and validation loss\n",
    "    weights_history.append(weights)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print('Lambda:', lambda_)\n",
    "    print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "    print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# get the best parameters\n",
    "best_idx = np.argmin(val_loss_history)\n",
    "best_lambda_ = lambdas[best_idx]\n",
    "weights = weights_history[best_idx]\n",
    "print('Best lambda:', best_lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_pred = predict_labels(weights, nor_x_test)\n",
    "OUTPUT_PATH = 'rr.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change y label from (1, -1) to (1, 0)\n",
    "lg_y_train = np.where(y_train == -1, 0, y_train)\n",
    "lg_y_val = np.where(y_val == -1, 0, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using gradient descent\n",
      "Gamma: 1\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7507\n",
      "Validation Loss: 0.4979 - Validation Accuracy: 0.7495\n",
      "Gamma: 0.75\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7507\n",
      "Validation Loss: 0.4979 - Validation Accuracy: 0.7496\n",
      "Gamma: 0.5\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7507\n",
      "Validation Loss: 0.4980 - Validation Accuracy: 0.7497\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "initial_w = np.zeros(nor_x_train.shape[1])\n",
    "max_iters = 1000\n",
    "gammas = [1, 0.75, 0.5]\n",
    "\n",
    "# create history of weights and validation loss\n",
    "weights_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "print('Logistic regression using gradient descent')\n",
    "for gamma in gammas:\n",
    "    # train model, get weights and loss\n",
    "    weights, loss = logistic_regression(lg_y_train, nor_x_train, initial_w, max_iters, gamma)\n",
    "    val_loss = compute_lg_loss(lg_y_val, nor_x_val, weights)\n",
    "\n",
    "    # make prediction\n",
    "    y_train_pred = predict_lg_labels(weights, nor_x_train)\n",
    "    y_val_pred = predict_lg_labels(weights, nor_x_val)\n",
    "\n",
    "    # compute accuracy\n",
    "    train_acc = compute_accuracy(lg_y_train, y_train_pred)\n",
    "    val_acc = compute_accuracy(lg_y_val, y_val_pred)\n",
    "    \n",
    "    # store weights and validation loss\n",
    "    weights_history.append(weights)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print('Gamma:', gamma)\n",
    "    print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "    print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma: 1\n"
     ]
    }
   ],
   "source": [
    "# get the best parameters\n",
    "best_idx = np.argmin(val_loss_history)\n",
    "best_gamma = gammas[best_idx]\n",
    "weights = weights_history[best_idx]\n",
    "print('Best gamma:', best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_pred = predict_lg_labels(weights, nor_x_test)\n",
    "y_test_pred[y_test_pred == 0] = -1\n",
    "OUTPUT_PATH = 'lg_gd.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using gradient descent\n",
      "Lambda: 0.01 - Gamma: 1\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7310\n",
      "Validation Loss: 0.5432 - Validation Accuracy: 0.7299\n",
      "Lambda: 0.01 - Gamma: 0.75\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7467\n",
      "Validation Loss: 0.5056 - Validation Accuracy: 0.7463\n",
      "Lambda: 0.01 - Gamma: 0.5\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7467\n",
      "Validation Loss: 0.5056 - Validation Accuracy: 0.7464\n",
      "Lambda: 0.005 - Gamma: 1\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7431\n",
      "Validation Loss: 0.5195 - Validation Accuracy: 0.7411\n",
      "Lambda: 0.005 - Gamma: 0.75\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7486\n",
      "Validation Loss: 0.5014 - Validation Accuracy: 0.7470\n",
      "Lambda: 0.005 - Gamma: 0.5\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7486\n",
      "Validation Loss: 0.5014 - Validation Accuracy: 0.7471\n",
      "Lambda: 0.001 - Gamma: 1\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7503\n",
      "Validation Loss: 0.4982 - Validation Accuracy: 0.7489\n",
      "Lambda: 0.001 - Gamma: 0.75\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7502\n",
      "Validation Loss: 0.4982 - Validation Accuracy: 0.7489\n",
      "Lambda: 0.001 - Gamma: 0.5\n",
      "Training Loss: 0.3400 - Training Accuracy: 0.7502\n",
      "Validation Loss: 0.4983 - Validation Accuracy: 0.7489\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "lambdas = [0.01, 0.005, 0.001]\n",
    "initial_w = np.zeros(nor_x_train.shape[1])\n",
    "max_iters = 1000\n",
    "gammas = [1, 0.75, 0.5]\n",
    "\n",
    "# create history of weights and validation loss\n",
    "weights_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "print('Logistic regression using gradient descent')\n",
    "for lambda_ in lambdas:\n",
    "    for gamma in gammas:\n",
    "        # train model, get weights and loss\n",
    "        weights, loss = reg_logistic_regression(lg_y_train, nor_x_train, lambda_, initial_w, max_iters, gamma)\n",
    "        val_loss = compute_lg_loss(lg_y_val, nor_x_val, weights)\n",
    "\n",
    "        # make prediction\n",
    "        y_train_pred = predict_lg_labels(weights, nor_x_train)\n",
    "        y_val_pred = predict_lg_labels(weights, nor_x_val)\n",
    "\n",
    "        # compute accuracy\n",
    "        train_acc = compute_accuracy(lg_y_train, y_train_pred)\n",
    "        val_acc = compute_accuracy(lg_y_val, y_val_pred)\n",
    "        \n",
    "        # store weights and validation loss\n",
    "        weights_history.append(weights)\n",
    "        val_loss_history.append(val_loss)\n",
    "\n",
    "        print(\"Lambda: {} - Gamma: {}\".format(lambda_, gamma))\n",
    "        print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "        print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda: 0.001 - Best gamma: 1\n"
     ]
    }
   ],
   "source": [
    "# get the best parameters\n",
    "best_idx = np.argmin(val_loss_history)\n",
    "best_lambda_ = lambdas[int(best_idx / len(lambdas))]\n",
    "best_gamma = gammas[best_idx % len(gammas)]\n",
    "weights = weights_history[best_idx]\n",
    "print(\"Best lambda: {} - Best gamma: {}\".format(best_lambda_, best_gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_pred = predict_lg_labels(weights, nor_x_test)\n",
    "y_test_pred[y_test_pred == 0] = -1\n",
    "OUTPUT_PATH = 'lg_sgd.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 30)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAELCAYAAADDZxFQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAepUlEQVR4nO3df5xVdb3v8de7ITLFDlSUyfBTFPEHDjma1sPyx1WIDM+jvDrjvUfBipMJ3cNR7/U8Ug4XO8Utcx4Z6jmUaRnNaGSKhpons8MxDAZFERBBQBmohJRjlEKMn/vHXoN7Ztae2TPMmj0/3s/HYz/Y3+/6rrU/X7czn1nftdb3q4jAzMyspXeUOgAzM+uZnCDMzCyVE4SZmaVygjAzs1ROEGZmlsoJwszMUmWaICRNlrRB0iZJ16Zsr5G0Onm9IGl33rYRkn4hab2kdZJGZRmrmZk1p6yeg5BUBrwAnAs0ACuB6ohYV6D9LGBiRFyelB8H/iUiHpU0CHgrIv6SSbBmZtZKlmcQpwKbImJzROwD6oAL2mhfDdQCSDoOGBARjwJExB4nBzOz7jUgw2MPA7bllRuAj6Q1lDQSGA08llQdA+yWdG9S/+/AtRHR2GK/GcAMgMMOO+zkY489tks7YGbW161atWpXRAxN25ZlglBKXaHxrCpgcV4CGACcAUwEXgbuBqYBtzc7WMRCYCFAZWVl1NfXH3zUZmb9iKSXCm3LcoipARieVy4HdhRoW0UyvJS379PJ8NR+4D7gw5lEaWZmqbJMECuBoyWNljSQXBJY0rKRpHHAEGB5i32HSGo67TkbSL24bWZm2cgsQSR/+c8EHgHWA/dExFpJ8yRNzWtaDdRF3u1UyVDT1cAvJa0hN1z13axiNTOz1jK7zbW7+RqEmVnHSVoVEZVp2/wktZmZpXKCMDOzVE4QZmaWygnCzMxSOUGYmVkqJwgzM0vlBGFmZqmcIMzMLJUThJmZpXKCMDOzVE4QZmaWygnCzMxSOUGYmVkqJwgz61YPP/ww48aNY+zYscyfP7/V9tmzZ1NRUUFFRQXHHHMMgwcPPrCtrKzswLapU99eNWDBggWMHTsWSezatatb+pGvL/YJgIjoE6+TTz45zKxn279/f4wZMyZefPHF2Lt3b0yYMCHWrl1bsP3NN98c06dPP1A+7LDDUts99dRTsWXLlhg5cmTs3Lmzy+NuS2/vE1AfBX6v+gzCzLrNihUrGDt2LGPGjGHgwIFUVVVx//33F2xfW1tLdXV1u8edOHEio0aN6sJIi9cX+9TECcLMus327dsZPvztperLy8vZvn17atuXXnqJLVu2cPbZZx+oe/PNN6msrOS0007jvvvuyzzeYvTFPjUZUOoAzKz/iJQVLCWltq2rq+PCCy+krKzsQN3LL7/MkUceyebNmzn77LM58cQTOeqoozKLtxh9sU9NfAZhZt2mvLycbdu2HSg3NDRw5JFHpratq6trNRTT1HbMmDGceeaZPP3009kFW6S+2KcmThBm1m1OOeUUNm7cyJYtW9i3bx91dXXN7txpsmHDBl577TVOP/30A3WvvfYae/fuBWDXrl088cQTHHfccd0WeyF9sU9NnCDMrNsMGDCABQsWMGnSJMaPH89FF13E8ccfz5w5c1iyZMmBdrW1tVRVVTUbqlm/fj2VlZWcdNJJnHXWWVx77bUHfpnefPPNlJeX09DQwIQJE/j85z/vPnUBpY2f9UaVlZVRX19f6jDMzHoVSasiojJtm88gzMwslROEmZmlcoIwM7NUThBmZpbKCcLMzFJl+iS1pMnAt4Ey4HsRMb/F9hrgrKR4KPCBiBicbGsE1iTbXo6I1jcWm1mPNe+qB0sdQqo53zr/oPZf8E/TuyiSrjPz63dkctzMEoSkMuAW4FygAVgpaUlErGtqExGz89rPAibmHeKNiKjIKj4zM2tblkNMpwKbImJzROwD6oAL2mhfDdRmGI+ZmXVAlgliGLAtr9yQ1LUiaSQwGngsr/oQSfWSnpT0t9mFaWZmabK8BpE2nWGhx7argMUR0ZhXNyIidkgaAzwmaU1EvNjsA6QZwAyAESNGdEXMZmaWyPIMogEYnlcuB3YUaFtFi+GliNiR/LsZeJzm1yea2iyMiMqIqBw6dGhXxGxmZoksE8RK4GhJoyUNJJcElrRsJGkcMARYnlc3RNK7kvfvBz4GrGu5r5mZZSezBBER+4GZwCPAeuCeiFgraZ6k/FtWq4G6aD5r4HigXtIzwK+A+fl3P5m1dDCLxgO8/vrrDBs2jJkzZx6oO/PMMxk3btyB/V555ZXM+5GvL/bJepdMn4OIiKXA0hZ1c1qU56bs9xvgxCxjs76jsbGRK6+8kkcffZTy8nJOOeUUpk6d2mxe/ZqamgPvv/Od77RalOX666/nE5/4RKtjL1q0iMrK1IkuM9UX+2S9j5+ktl7vYBeNX7VqFX/4wx8477zzuiPcovTFPlnv4wRhvd7BLBr/1ltvcdVVV/HNb34ztf306dOpqKjghhtuSF17OCt9sU/W+zhBtCGLMeAmU6dO5YQTTsgs9v7kYBaNv/XWW5kyZUqzX8ZNFi1axJo1a1i2bBnLli3jrrvu6trA29AX+2S9T6bXIHqzLMeA7733XgYNGpRd8P1MRxeNv+WWWw6Uly9fzrJly7j11lvZs2cP+/btY9CgQcyfP59hw3LPdR5++OFccsklrFixgksvvTTbziT6Yp+s9/EZRAFZjQHv2bOHm266ieuuuy6z2Pubg1k0ftGiRbz88sts3bqVG2+8kUsvvZT58+ezf/9+du3aBcBf//pXHnzwwW494+uLfbLexwmigKzGgK+//nquuuoqDj300GwC74cOZtH4Qvbu3cukSZOYMGECFRUVDBs2jC984QtZdqOZvtgn6308xFRAFmPAq1evZtOmTdTU1LB169Yuj7k/mzJlClOmTGlWN2/evGbluXPntnmMadOmMW3aNAAOO+wwVq1a1ZUhdlhf7JP1Lk4QBWQxBjxy5EhWrVrFqFGj2L9/P6+88gpnnnkmjz/+eNbdMTPrMCeIAvLHgIcNG0ZdXR0//vGPW7UrNAbc5M4776S+vv7AXVBXXHEFAFu3buX88893cjCzHsvXIArIYgzYzKw3UV95UKaysjLq6+tLHYaZJbzkaPc5mCVHJa2KiNS5V3wGYWZmqZwgzMwslS9SW69y0d1XlDqEVPdcfNtB7f/EBZ/toki6zsfu/2mpQ7AS8xmEmZmlcoIwM7NUHmJKfPqqwvMslcoD37qg1CGYWT/mMwgzM0vlBGFmZqmcIMzMLJUThJmZpXKCMDOzVE4QZmaWygnCzMxSOUGYmVkqJwgzM0vlBGFmZqmcIMzMLFWmCULSZEkbJG2SdG3K9hpJq5PXC5J2t9j+HknbJS3IMk4zM2sts8n6JJUBtwDnAg3ASklLImJdU5uImJ3XfhYwscVhbgB+nVWMZmZWWJZnEKcCmyJic0TsA+qAtqYnrQZqmwqSTgY+CPwiwxjNzKyALBPEMGBbXrkhqWtF0khgNPBYUn4H8C3gmrY+QNIMSfWS6nfu3NklQZuZWU6WCUIpdVGgbRWwOCIak/KXgKURsa1A+9zBIhZGRGVEVA4dOvQgQjUzs5ayXDCoARieVy4HdhRoWwVcmVc+HThD0peAQcBASXsiotWFbjMzy0aWCWIlcLSk0cB2ckngkpaNJI0DhgDLm+oi4n/kbZ8GVDo5mJl1r8yGmCJiPzATeARYD9wTEWslzZM0Na9pNVAXEYWGn8zMrAQyXZM6IpYCS1vUzWlRntvOMe4E7uzi0MzMrB1+ktrMzFI5QZiZWSonCDMzS+UEYWZmqZwgzMwslROEmZmlcoIwM7NUThBmZpbKCcLMzFI5QZiZWap2E4SkmZKGdEcwZmbWcxRzBnEEueVC70nWmE5b58HMzPqYdhNERFwHHA3cDkwDNkr6mqSjMo7NzMxKqKhrEMlU3L9PXvvJrd+wWNI3MozNzMxKqN3pviV9GbgM2AV8D7gmIv6arBu9Efjf2YZoZmalUMx6EO8HPhMRL+VXRsRbks7PJiwzMyu1YoaYlgKvNhUkHS7pIwARsT6rwMzMrLSKSRC3AXvyyn9O6szMrA8rJkEof73oiHiLjJcqNTOz0ismQWyW9GVJ70xe/wvYnHVgZmZWWsUkiC8CHwW2Aw3AR4AZWQZlZmal1+5QUUS8AlR1QyxmZtaDFPMcxCHA54DjgUOa6iPi8gzjMjOzEitmiOkucvMxTQJ+DZQDf8oyKDMzK71iEsTYiLge+HNE/AD4FHBitmGZmVmpFZMg/pr8u1vSCcDfAKMyi8jMzHqEYp5nWJisB3EdsAQYBFyfaVRmZlZybSaIZEK+1yPiNeA/gDEdObikycC3gTLgexExv8X2GuCspHgo8IGIGCxpJHBvst87ge9ExL925LPNzOzgtJkgkgn5ZgL3dPTAksqAW4BzyT0/sVLSkohYl3f82XntZwETk+LvgI9GxF5Jg4Dnkn13dDQOMzPrnGKuQTwq6WpJwyW9t+lVxH6nApsiYnNE7APqgAvaaF8N1AJExL6I2JvUv6vIOM3MrAsVcw2i6XmHK/PqgvaHm4YB2/LKTU9ht5IMKY0GHsurGw78HBhLbg2KVmcPkmaQPNU9YsSIdsIxM7OOKOZJ6tGdPHba2tWRUge5J7UXR0Rj3uduAyZIOhK4T9LiiPhDi9gWAgsBKisrCx3bzMw6oZgnqS9Nq4+IH7azawMwPK9cDhS6hlBF8zOU/M/ZIWktcAawuJ3PNDOzLlLMENMpee8PAc4BngLaSxArgaMljSY30V8VcEnLRpLGkVvjenleXTnwx4h4I7nF9mPATUXEamZmXaSYIaZZ+WVJf0Nu+o329tuf3AH1CLnbVb8fEWslzQPqI2JJ0rQaqMtfcwIYD3xLUpAbqroxItYU1SMzM+sSnVn45y/A0cU0jIil5JYsza+b06I8N2W/R4EJnYjNzMy6SDHXIB7g7YvL7wCOoxPPRZiZWe9SzBnEjXnv9wMvRURDRvGYmVkPUUyCeBn4XUS8CSDp3ZJGRcTWTCMzM7OSKuYJ5Z8Ab+WVG5M6MzPrw4pJEAOSqTKA3DQYwMDsQjIzs56gmASxU9LUpoKkC4Bd2YVkZmY9QTHXIL4ILJK0ICk3AKlPV5uZWd9RzINyLwKnJdNuKyK8HrWZWT/Q7hCTpK9JGhwReyLiT5KGSPpqdwRnZmalU8w1iE9GxO6mQrK63JTsQjIzs56gmARRJuldTQVJ7ya3iI+ZmfVhxVyk/hHwS0l3JOXpwA+yC8nMzHqCYi5Sf0PSs8B/Izez6sPAyKwDMzOz0ip2reffk3ua+rPk1oNYn1lEZmbWIxQ8g5B0DLlFfqqBPwJ3k7vN9axuis3MzEqorSGm54FlwKcjYhOApNndEpWZmZVcW0NMnyU3tPQrSd+VdA65axBmZtYPFEwQEfGziLgYOBZ4HJgNfFDSbZLO66b4zMysRNq9SB0Rf46IRRFxPlAOrAauzTwyMzMrqWLvYgIgIl6NiH+LiLOzCsjMzHqGDiUIMzPrP5wgzMwslROEmZmlcoIwM7NUThBmZpbKCcLMzFI5QZiZWapME4SkyZI2SNokqdXDdZJqJK1OXi9I2p3UV0haLmmtpGclXZxlnGZm1loxCwZ1iqQy4BbgXKABWClpSUSsa2oTEbPz2s8CJibFvwCXRsRGSUcCqyQ9kr/0qZmZZSvLM4hTgU0RsTki9gF1wAVttK8GagEi4oWI2Ji83wG8AgzNMFYzM2shywQxDNiWV25I6lqRNBIYDTyWsu1UYCDwYsq2GZLqJdXv3LmzS4I2M7OcLBNE2tTgUaBtFbA4IhqbHUD6EHAXMD0i3mp1sIiFEVEZEZVDh/oEw8ysK2WZIBqA4XnlcmBHgbZVJMNLTSS9B/g5cF1EPJlJhGZmVlCWCWIlcLSk0ZIGkksCS1o2kjQOGAIsz6sbCPwM+GFE/CTDGM3MrIDMEkRE7AdmAo8A64F7ImKtpHmSpuY1rQbqIiJ/+Oki4OPAtLzbYCuyitXMzFrL7DZXgIhYCixtUTenRXluyn4/An6UZWxmZtY2P0ltZmapnCDMzCyVE4SZmaVygjAzs1ROEGZmlsoJop95+OGHGTduHGPHjmX+/Pmtts+ePZuKigoqKio45phjGDx48IFtkydPZvDgwZx//vnN9vnc5z7HSSedxIQJE7jwwgvZs2dP5v0ws+w5QfQjjY2NXHnllTz00EOsW7eO2tpa1q1b16xNTU0Nq1evZvXq1cyaNYvPfOYzB7Zdc8013HXXXa2OW1NTwzPPPMOzzz7LiBEjWLBgQeZ9MbPsOUH0IytWrGDs2LGMGTOGgQMHUlVVxf3331+wfW1tLdXV1QfK55xzDocffnirdu95z3sAiAjeeOMNpLRpuMyst3GC6Ee2b9/O8OFvT49VXl7O9u3bU9u+9NJLbNmyhbPPPruoY0+fPp0jjjiC559/nlmzZnVJvGZWWk4Q/Ujz2UxyCv21X1dXx4UXXkhZWVlRx77jjjvYsWMH48eP5+677z6oOM2sZ3CC6EfKy8vZtu3tJToaGho48sgjU9vW1dU1G14qRllZGRdffDE//elPDypOM+sZnCD6kVNOOYWNGzeyZcsW9u3bR11dHVOnTm3VbsOGDbz22mucfvrp7R4zIti0adOB9w888ADHHntsl8duZt0v08n6rGcZMGAACxYsYNKkSTQ2NnL55Zdz/PHHM2fOHCorKw8ki9raWqqqqloNP51xxhk8//zz7Nmzh/Lycm6//XbOPfdcLrvsMl5//XUigpNOOonbbrutFN0zsy7mBNHPTJkyhSlTpjSrmzdvXrPy3LlzU/ddtmxZav0TTzzRJbGZWc/iISYzM0vlBGFmZqmcIMzMLJUThJmZpXKCMDOzVE4QZmaWyre59mGb/+WzpQ6hlTFf8VPWZr2FzyDMzCyVE4SZmaVygjAzs1ROEGZmlsoJwszMUjlBmJlZqkwThKTJkjZI2iTp2pTtNZJWJ68XJO3O2/awpN2SHswyRjMzS5fZcxCSyoBbgHOBBmClpCURsa6pTUTMzms/C5iYd4hvAocCf59VjGZmVliWZxCnApsiYnNE7APqgAvaaF8N1DYVIuKXwJ8yjM/MzNqQZYIYBmzLKzckda1IGgmMBh7LMB4zM+uALBOEUuqiQNsqYHFENHboA6QZkuol1e/cubPDAZqZWWFZJogGYHheuRzYUaBtFXnDS8WKiIURURkRlUOHDu1EiGZmVkiWCWIlcLSk0ZIGkksCS1o2kjQOGAIszzAWMzProMwSRETsB2YCjwDrgXsiYq2keZKm5jWtBuoiotnwk6RlwE+AcyQ1SJqUVaxmZtZaptN9R8RSYGmLujktynML7HtGdpGZmVl7/CS1mZmlcoIwM7NUThBmZpbKCcLMzFI5QZiZWSonCDMzS+UEYWZmqZwgzMwslROEmZmlcoIwM7NUThBmZpbKCcLMzFI5QZiZWSonCDMzS+UEYWZmqZwgzMwslROEmZmlcoIwM7NUThBmZpbKCcLMzFI5QZiZWSonCDMzS+UEYWZmqZwgzMwslROEmZmlcoIwM7NUThBmZpbKCcLMzFJlmiAkTZa0QdImSdembK+RtDp5vSBpd962yyRtTF6XZRmnmZm1NiCrA0sqA24BzgUagJWSlkTEuqY2ETE7r/0sYGLy/r3APwOVQACrkn1fyypeMzNrLssziFOBTRGxOSL2AXXABW20rwZqk/eTgEcj4tUkKTwKTM4wVjMza0ERkc2BpQuByRHx+aT8d8BHImJmStuRwJNAeUQ0SroaOCQivppsvx54IyJubLHfDGBGUhwHbMikMx3zfmBXqYPIQF/sV1/sE/TNfrlP2RkZEUPTNmQ2xAQopa5QNqoCFkdEY0f2jYiFwMLOhZcNSfURUVnqOLpaX+xXX+wT9M1+uU+lkeUQUwMwPK9cDuwo0LaKt4eXOrqvmZllIMsEsRI4WtJoSQPJJYElLRtJGgcMAZbnVT8CnCdpiKQhwHlJnZmZdZPMhpgiYr+kmeR+sZcB34+ItZLmAfUR0ZQsqoG6yLsYEhGvSrqBXJIBmBcRr2YVaxfrUUNeXagv9qsv9gn6Zr/cpxLI7CK1mZn1bn6S2szMUjlBmJlZKicI6xck7UmpGyfp8WSql/WSevyYcD5JjUnsz0l6QNLgpH6UpDeSbesk/VDSO0sdbzHSvqek/n9KelbSWknPSPpeU397ona+m+cK7POPkp6XtCbp402l/t6cIFIU+GUyV9L2vB+66iKOU/ALl7Q1qV+THO+rkt7V0/oh6TRJv837JTo3b9tkSSuSPq6WdLekEcm2OyVtSfr9QvJLalhX9q8L3AzURERFRIwHvlPqgDrojST2E4BXgSvztr0YERXAieRuE7+oFAF2BUmTgdnAJyPieODDwG+AD5Y0sLa19d20IumL5O7WPC0iTgROAV4B3p15pG1wguiYmuSH7gLg39rK7kV+4Wcl204FxtB9dzUU3Q/gB8CMpP0JwD0Akk4g9wv1sog4Ntm+CBiVt+81EXESuafcnwZ+ldzy3FN8iNwzNwBExJoSxnKwlgOtEnDy8OmKtG29yFeAqyNiO+T6FBHfj4ieMHNCMVK/mxa+AlwREbsBImJfRMyPiNczj64NThCdEBEbgb+Qe36jkKK/8IjYA3wR+NtkosJuUWQ/PgD8LmnfmDfZ4v8BvhYR6/OOtyQi/iPlcyIiaoDfA5/sqvi7QA3wmKSHJM3uyUMWbUkmxjyH9OeMDgE+Ajzc3XF1oeOBp0odRGe09d3ktTkcGBQRW7otsCI5QXSCpA8DGyPilQLbO/yFJ4ljC3B010TZvvb6kagBNkj6maS/T37hQOd+aJ8Cju1EqJmIiDuA8cBPgDOBJ7t6mC9j75a0Gvgj8F5yk1o2OSpv28sR8WwpAuxqkk5MhjNflHRxqeNpQ1vfTUsibyohSZOSPm6V9NGM42yTE0THzJa0AfgtMLeNdp39wtPmoMpCsf0gIuaRm3b9F8AlpPwlKul9entNj6vbOFx39a9oEbEjGa64ANhPbhitt3gjGdobCQwk/RrEWOA0SVNLEWAXWUvuugMRsSbp10OUeHy+HW19N80kfxz+WdLopPxIsu9zyb4l4wTRMTURMQ64GPhh3l/TzXTmC0/OOkYBL2QReAtF9aNJRLwYEbeRO1U+SdL7aP5D+8ekfwuBQW0caiKwvo3t3Sq5yN5008ARwPuA7aWNquMi4r+ALwNXt7yeFBG/A64F/qkUsXWRrwM3SirPq+vJyeGAtr6bFr4O3JZ3t5OANn8uu4MTRCdExL1APdDWSndFf+GSBgG3Avd156JIxfRD0qeS2CE3/NUI7Aa+AXxF0vi85ocWOIYkfZncReFSjYUfKqkh7/WP5G4ieE7SM+SmhLkmIn5fovgOSkQ8DTxDbs6zlu4j1/8zujeqTmn1PUXEUnJ3nD2U3Hn3G3L/H/aK+dlSvptxLfr434HbgH8HfivpWeAJcjd2PF2SoBOeaiOFpLdoPnvsTcB7gD1Na1JIOhn4MTA+It5KOYaAq4AvAHuBPeS+9K9GxH9J2gr8idywyzuAnwE3RMSbPawfdeTOFP5CbgjmKxHxSLLtU+SGqA4nGesG/jkiXpB0J/AJ4HVyieNJ4J8ioqHlZ5hZz+QEYWZmqTzEZGZmqbJcUa5fkHQL8LEW1d9ObqHsNfpKP8ys63iIyczMUnmIyczMUjlBmJlZKicI63ckhaS78soDJO2U9GBSniZpQTvHeFzSy3nPiCDpPhWYrjqvzWBJX8orn9n0uZ3sy0Htb9YWJwjrj/4MnCCp6Wncc+ncE9S7SS7sJw9EfqiIfQYDX2q3lVkP4ARh/dVDwKeS99VAbSeOUcfbT8d+Brg3f6OkayStVG6hm/+bVM8nmUhP0jeTukGSFiu3rsaiprMSSedIelq5NUO+3zSRYDJFyPOS/jP53KbP+0Ry3NXJfod3ok9mBzhBWH9VB1Ql81BNIDdxYUf9Evi4clM6VwF3N22QdB65qUlOBSqAkyV9nNy8SC8mi8lckzSfCPwDcBy5dUE+lsR1J3BxsmbIAOCKpP67wKeBM4Aj8uK5GrgymRfrDOCNTvTJ7AAnCOuXkumvR5E7e1jaycM0Av9JbtLDd0fE1rxt5yWvp3l7mvNCU7mviIiGZKqT1Ulc44AtEdE0eeMPgI8nx9kSERsjd4/6j/KO8wRwUzLv1eCI2N/JfpkBThDWvy0BbqRzw0tN6sitrHdPi3oBX0/OFCoiYmxE3F7gGHvz3jeSO1toa2r01IeXImI+8HlyM50+KanHrL1hvZMThPVn3wfmHeRSo8vIzdzbMsk8AlyezNSLpGGSPkBugsZirg08D4ySNDYp/x3w66R+tKSjkvoDa4pLOipZL+H/kZul1wnCDooThPVbybDOtwtsntZiSubytEbJcqo3RsSuFvW/IDdL7nJJa4DFwOER8UfgCUnP5V2kTjvum8B04CfJ/m8B/5rUzwB+nlykfilvt39IjvsMuesPDxXz38GsEE+1YWZmqXwGYWZmqZwgzMwslROEmZmlcoIwM7NUThBmZpbKCcLMzFI5QZiZWar/D+TzQFQztNnsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_test_accs = [0.744, 0.713, 0.745, 0.745, 0.751, 0.751]\n",
    "x = np.arange(len(best_test_accs))\n",
    "plt.bar(x, best_test_accs, width=0.7, color=sns.color_palette('deep'))\n",
    "for i, v in enumerate(best_test_accs):\n",
    "    plt.text(x[i] - 0.28, v + 0.002, str(v))\n",
    "plt.xticks(x, ('LR_GD', 'LR_SGD', 'LS', 'RR', 'LG', 'RLG'))\n",
    "plt.ylim(bottom=0.7, top=0.76)\n",
    "plt.xlabel('ML Methods')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression - with preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0, x_1, x_2, x_9, y_0, y_1, y_2, y_9 = LG_load_train_data_split(DATA_TRAIN_PATH = './data/train.csv')\n",
    "test_x_0, test_x_1, test_x_2, test_x_9, id_0, id_1, id_2, id_9 = LG_load_test_data_split(DATA_TEST_PATH = './data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_penalized_gradient_descent(y, x, lambda_, degree, gamma, threshold, max_iter):\n",
    "    losses = []\n",
    "    tx = build_poly(x, degree)\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        \n",
    "        if iter % 5000 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss.item()))\n",
    "        \n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break         \n",
    "    \n",
    "    print(\"loss={l}, lambda={k}\".format(l=calculate_loss(y, tx, w).item(), k=lambda_))\n",
    "    return calculate_loss(y, tx, w), w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best lambda by cross validation\n",
    "#lambda_0 = cross_validation_select_lambda(x_0, y_0, 2, 4, seed = 1)\n",
    "#lambda_1 = cross_validation_select_lambda(x_1, y_1, 3, 4, seed = 1)\n",
    "#lambda_2 = cross_validation_select_lambda(x_2, y_2, 3, 4, seed = 1)\n",
    "#lambda_9 = cross_validation_select_lambda(x_9, y_9, 3, 4, seed = 1)\n",
    "\n",
    "# report the best lambda here\n",
    "lambda_0 = 0\n",
    "lambda_1 = 1e-5\n",
    "lambda_2 = 1e-4\n",
    "lambda_9 = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(y):\n",
    "    y = np.reshape(y, (y.shape[0], 1))\n",
    "    return y\n",
    "# reshape y and change y label\n",
    "y_0 = transform(y_0)\n",
    "y_1 = transform(y_1)\n",
    "y_2 = transform(y_2)\n",
    "y_9 = transform(y_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate prediction to test the local accuracy and create submission\n",
    "def predict(x, w, degree, generate_submission):\n",
    "    test = build_poly(x, degree)\n",
    "    prediction = sigmoid(test.dot(w))\n",
    "    if(generate_submission):\n",
    "        prediction[np.where(prediction <= 0.5)] = 1\n",
    "        prediction[np.where(prediction > 0.5)] = -1\n",
    "    else:\n",
    "        prediction[np.where(prediction <= 0.5)] = 0 \n",
    "        prediction[np.where(prediction > 0.5)] = 1\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0\n",
      "Current iteration=0, loss=51147.33045351836\n",
      "Current iteration=5000, loss=32618.187217544502\n",
      "Current iteration=10000, loss=32391.637662442154\n",
      "Current iteration=15000, loss=32245.80615683664\n",
      "Current iteration=20000, loss=32145.58644255044\n",
      "Current iteration=25000, loss=32075.372749986083\n",
      "loss=32049.174640625875, lambda=0\n",
      "training accuracy:  [0.80283236]\n",
      "\n",
      "Group 1\n",
      "Current iteration=0, loss=48507.82598994609\n",
      "Current iteration=5000, loss=33033.20935363068\n",
      "loss=33132.32232789036, lambda=1e-05\n",
      "training accuracy:  [0.78527336]\n",
      "\n",
      "Group 2\n",
      "Current iteration=0, loss=47213.02705666011\n",
      "Current iteration=5000, loss=29159.839277081042\n",
      "Current iteration=10000, loss=28780.021607563016\n",
      "Current iteration=15000, loss=28685.232175798978\n",
      "loss=28667.395463819426, lambda=0.0001\n",
      "training accuracy:  [0.81704202]\n",
      "\n",
      "Group 3\n",
      "Current iteration=0, loss=26418.611639861756\n",
      "Current iteration=5000, loss=7608.581695858295\n",
      "loss=7550.900612394515, lambda=0.0001\n",
      "training accuracy:  [0.93306921]\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter has been tuned and reported here\n",
    "max_iter = 80000\n",
    "gamma = 1e-7\n",
    "threshold = 0.01\n",
    "\n",
    "print('Group 0')\n",
    "_, w_0 = logistic_regression_penalized_gradient_descent(y_0, x_0.values, lambda_0, 2, gamma, threshold, max_iter)\n",
    "y_train_pred = predict(x_0.values, w_0, 2, False)\n",
    "train_acc = compute_accuracy(y_0, y_train_pred)\n",
    "print(\"training accuracy: \", train_acc)\n",
    "\n",
    "print('\\nGroup 1')\n",
    "_, w_1 = logistic_regression_penalized_gradient_descent(y_1, x_1.values, lambda_1, 3, gamma, threshold, max_iter)\n",
    "y_train_pred = predict(x_1.values, w_1, 3, False)\n",
    "train_acc = compute_accuracy(y_1, y_train_pred)\n",
    "print(\"training accuracy: \", train_acc)\n",
    "\n",
    "print('\\nGroup 2')\n",
    "_, w_2 = logistic_regression_penalized_gradient_descent(y_2, x_2.values, lambda_2, 3, gamma, threshold, max_iter)\n",
    "y_train_pred = predict(x_2.values, w_2, 3, False)\n",
    "train_acc = compute_accuracy(y_2, y_train_pred)\n",
    "print(\"training accuracy: \", train_acc)\n",
    "\n",
    "print('\\nGroup 3')\n",
    "_, w_9 = logistic_regression_penalized_gradient_descent(y_9, x_9.values, lambda_9, 3, gamma, threshold, max_iter)\n",
    "y_train_pred = predict(x_9.values, w_9, 3, False)\n",
    "train_acc = compute_accuracy(y_9, y_train_pred)\n",
    "print(\"training accuracy: \", train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_hat_0 = predict(test_x_0, w_0, 2, True)\n",
    "y_test_pred_0 = np.where(np.squeeze(y_test_hat_0.T) > 0.5, 1, -1)\n",
    "y_test_hat_1 = predict(test_x_1, w_1, 3, True)\n",
    "y_test_pred_1 = np.where(np.squeeze(y_test_hat_1.T) > 0.5, 1, -1)\n",
    "y_test_hat_2 = predict(test_x_2, w_2, 3, True)\n",
    "y_test_pred_2 = np.where(np.squeeze(y_test_hat_2.T) > 0.5, 1, -1)\n",
    "y_test_hat_9 = predict(test_x_9, w_9, 3, True)\n",
    "y_test_pred_9 = np.where(np.squeeze(y_test_hat_9.T) > 0.5, 1, -1)\n",
    "\n",
    "# concatenate three predicted results\n",
    "y_test_pred = np.concatenate((y_test_pred_0, y_test_pred_1), axis = 0)\n",
    "y_test_pred = np.concatenate((y_test_pred, y_test_pred_2), axis = 0)\n",
    "y_test_pred = np.concatenate((y_test_pred, y_test_pred_9), axis = 0)\n",
    "\n",
    "# concatenate the indexs of three group\n",
    "ids = np.concatenate((id_0, id_1), axis = 0)\n",
    "ids = np.concatenate((ids, id_2), axis = 0)\n",
    "ids = np.concatenate((ids, id_9), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids, y_test_pred, 'RLG_w_pre.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN - without preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change y label from (1, -1) to (1, 0)\n",
    "nn_x_train = nor_x_train[:, 1:]\n",
    "nn_x_val = nor_x_val[:, 1:]\n",
    "nn_x_test = nor_x_test[:, 1:]\n",
    "nn_y_train = np.where(y_train == -1, 0, y_train)\n",
    "nn_y_val = np.where(y_val == -1, 0, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00000 - 0.8s - loss: 0.4240 - acc: 0.8028 - val_loss: 0.3910 - val_acc: 0.8220\n",
      "Epoch: 00001 - 0.9s - loss: 0.3861 - acc: 0.8249 - val_loss: 0.3802 - val_acc: 0.8271\n",
      "Saving best model (epoch 2, val_acc: 0.8300)\n",
      "Saving best model (epoch 2, val_loss: 0.3745)\n",
      "Saving best model (epoch 3, val_acc: 0.8317)\n",
      "Saving best model (epoch 3, val_loss: 0.3713)\n",
      "Saving best model (epoch 4, val_loss: 0.3705)\n",
      "Epoch: 00005 - 3.4s - loss: 0.3693 - acc: 0.8333 - val_loss: 0.3736 - val_acc: 0.8301\n",
      "Saving best model (epoch 6, val_acc: 0.8319)\n",
      "Saving best model (epoch 6, val_loss: 0.3694)\n",
      "Saving best model (epoch 7, val_acc: 0.8339)\n",
      "Saving best model (epoch 8, val_acc: 0.8349)\n",
      "Saving best model (epoch 8, val_loss: 0.3661)\n",
      "Saving best model (epoch 10, val_loss: 0.3649)\n",
      "Epoch: 00010 - 4.2s - loss: 0.3639 - acc: 0.8362 - val_loss: 0.3649 - val_acc: 0.8347\n",
      "Saving best model (epoch 12, val_acc: 0.8356)\n",
      "Saving best model (epoch 14, val_acc: 0.8362)\n",
      "Saving best model (epoch 14, val_loss: 0.3638)\n",
      "Epoch: 00015 - 4.2s - loss: 0.3608 - acc: 0.8376 - val_loss: 0.3642 - val_acc: 0.8360\n",
      "Saving best model (epoch 16, val_acc: 0.8371)\n",
      "Saving best model (epoch 17, val_loss: 0.3634)\n",
      "Epoch: 00020 - 4.5s - loss: 0.3591 - acc: 0.8380 - val_loss: 0.3654 - val_acc: 0.8356\n",
      "Saving best model (epoch 25, val_acc: 0.8375)\n",
      "Saving best model (epoch 25, val_loss: 0.3632)\n",
      "Epoch: 00025 - 4.9s - loss: 0.3578 - acc: 0.8392 - val_loss: 0.3632 - val_acc: 0.8375\n",
      "Saving best model (epoch 29, val_loss: 0.3630)\n",
      "Epoch: 00030 - 5.0s - loss: 0.3566 - acc: 0.8398 - val_loss: 0.3659 - val_acc: 0.8358\n",
      "Saving best model (epoch 32, val_acc: 0.8375)\n",
      "Epoch: 00035 - 5.8s - loss: 0.3556 - acc: 0.8399 - val_loss: 0.3649 - val_acc: 0.8359\n",
      "Epoch: 00040 - 4.7s - loss: 0.3547 - acc: 0.8406 - val_loss: 0.3668 - val_acc: 0.8359\n",
      "Epoch: 00045 - 3.9s - loss: 0.3541 - acc: 0.8405 - val_loss: 0.3656 - val_acc: 0.8357\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "epochs = 50\n",
    "lr = 0.001\n",
    "batch_size = 250\n",
    "nn_architecture = [\n",
    "    {\"input_dim\": 30, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "\n",
    "# train model, get model parameters\n",
    "nn_params = train(nn_x_train.T, nn_y_train[np.newaxis,:], nn_x_val.T, nn_y_val[np.newaxis,:],\n",
    "                  nn_architecture, epochs, lr, batch_size, False, 0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_params = np.load('best_acc.npy', allow_pickle=True).item()\n",
    "# nn_params = np.load('best_loss.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep neural network\n",
      "Training Loss: 0.3528 - Training Accuracy: 0.8419\n",
      "Validation Loss: 0.3632 - Validation Accuracy: 0.8375\n"
     ]
    }
   ],
   "source": [
    "# get final output of the model\n",
    "y_train_hat, _ = full_forward_propagation(nn_x_train.T, nn_params, NN_ARCHITECTURE)\n",
    "y_val_hat, _ = full_forward_propagation(nn_x_val.T, nn_params, NN_ARCHITECTURE)\n",
    "\n",
    "# compute loss and accuracy\n",
    "train_loss = compute_nn_loss(y_train_hat, nn_y_train.reshape(nn_y_train.shape[0], 1).T)\n",
    "val_loss = compute_nn_loss(y_val_hat, nn_y_val.reshape(nn_y_val.shape[0], 1).T)\n",
    "\n",
    "train_acc = compute_nn_accuracy(y_train_hat, nn_y_train.reshape(nn_y_train.shape[0], 1).T)\n",
    "val_acc = compute_nn_accuracy(y_val_hat, nn_y_val.reshape(nn_y_val.shape[0], 1).T)\n",
    "\n",
    "print('Deep neural network')\n",
    "print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_hat, _ = full_forward_propagation(nn_x_test.T, nn_params, NN_ARCHITECTURE)\n",
    "y_test_pred = np.where(np.squeeze(y_test_hat.T) > 0.5, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'nn_wo_pre.csv'\n",
    "create_csv_submission(ids_test, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN - with preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0, x_1, x_2, y_0, y_1, y_2 = load_train_data_split(DATA_TRAIN_PATH = './data/train.csv')\n",
    "test_x_0, test_x_1, test_x_2, id_0, id_1, id_2 = load_test_data_split(DATA_TEST_PATH = './data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_0, y_val_0, tX_train_0, tX_val_0 = train_val_split(y_0, x_0.values, 0.2)\n",
    "y_train_1, y_val_1, tX_train_1, tX_val_1 = train_val_split(y_1, x_1.values, 0.2)\n",
    "y_train_2, y_val_2, tX_train_2, tX_val_2 = train_val_split(y_2, x_2.values, 0.2)\n",
    "\n",
    "# reshape data\n",
    "def transform(tX, y):\n",
    "    tX = tX.T\n",
    "    y = np.reshape(y, (1, y.shape[0]))\n",
    "    nn_y = np.where(y == -1, 0, y)\n",
    "    return tX, nn_y\n",
    "\n",
    "tX_train_0, y_train_0 = transform(tX_train_0, y_train_0)\n",
    "tX_val_0, y_val_0 = transform(tX_val_0, y_val_0)\n",
    "tX_train_1, y_train_1 = transform(tX_train_1, y_train_1)\n",
    "tX_val_1, y_val_1 = transform(tX_val_1, y_val_1)\n",
    "tX_train_2, y_train_2 = transform(tX_train_2, y_train_2)\n",
    "tX_val_2, y_val_2 = transform(tX_val_2, y_val_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00000 - 1.3s - loss: 0.3651 - acc: 0.8392 - val_loss: 0.3486 - val_acc: 0.8448\n",
      "Epoch: 00001 - 1.3s - loss: 0.3429 - acc: 0.8488 - val_loss: 0.3455 - val_acc: 0.8471\n",
      "Saving best model (epoch 2, val_loss: 0.3446)\n",
      "Saving best model (epoch 4, val_loss: 0.3421)\n",
      "Saving best model (epoch 5, val_acc: 0.8481)\n",
      "Saving best model (epoch 5, val_loss: 0.3410)\n",
      "Epoch: 00005 - 5.0s - loss: 0.3354 - acc: 0.8525 - val_loss: 0.3410 - val_acc: 0.8481\n",
      "Saving best model (epoch 8, val_loss: 0.3408)\n",
      "Epoch: 00010 - 5.4s - loss: 0.3320 - acc: 0.8539 - val_loss: 0.3410 - val_acc: 0.8479\n",
      "Saving best model (epoch 11, val_acc: 0.8490)\n",
      "Saving best model (epoch 11, val_loss: 0.3402)\n",
      "Epoch: 00015 - 4.8s - loss: 0.3299 - acc: 0.8544 - val_loss: 0.3427 - val_acc: 0.8478\n",
      "Saving best model (epoch 20, val_acc: 0.8491)\n",
      "Epoch: 00020 - 4.5s - loss: 0.3280 - acc: 0.8552 - val_loss: 0.3419 - val_acc: 0.8491\n",
      "Epoch: 00025 - 4.6s - loss: 0.3260 - acc: 0.8556 - val_loss: 0.3443 - val_acc: 0.8482\n",
      "Epoch: 00030 - 4.4s - loss: 0.3239 - acc: 0.8566 - val_loss: 0.3426 - val_acc: 0.8472\n",
      "Epoch: 00035 - 4.5s - loss: 0.3224 - acc: 0.8572 - val_loss: 0.3457 - val_acc: 0.8481\n",
      "Epoch: 00040 - 4.5s - loss: 0.3206 - acc: 0.8597 - val_loss: 0.3448 - val_acc: 0.8465\n",
      "Epoch: 00045 - 4.6s - loss: 0.3187 - acc: 0.8597 - val_loss: 0.3478 - val_acc: 0.8453\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "# train model 0, get model parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "nn_architecture_0 = [\n",
    "    {\"input_dim\": 18, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "param_0 = train(tX_train_0, y_train_0, tX_val_0, y_val_0, nn_architecture_0, epochs, learning_rate, batch_size, True, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00000 - 0.8s - loss: 0.4546 - acc: 0.7858 - val_loss: 0.4243 - val_acc: 0.8064\n",
      "Epoch: 00001 - 0.7s - loss: 0.4116 - acc: 0.8137 - val_loss: 0.4144 - val_acc: 0.8112\n",
      "Saving best model (epoch 2, val_loss: 0.4112)\n",
      "Saving best model (epoch 3, val_acc: 0.8145)\n",
      "Saving best model (epoch 3, val_loss: 0.4086)\n",
      "Saving best model (epoch 4, val_acc: 0.8147)\n",
      "Saving best model (epoch 4, val_loss: 0.4069)\n",
      "Epoch: 00005 - 2.9s - loss: 0.3936 - acc: 0.8216 - val_loss: 0.4095 - val_acc: 0.8136\n",
      "Saving best model (epoch 6, val_acc: 0.8150)\n",
      "Saving best model (epoch 6, val_loss: 0.4069)\n",
      "Saving best model (epoch 8, val_loss: 0.4055)\n",
      "Saving best model (epoch 9, val_acc: 0.8164)\n",
      "Saving best model (epoch 9, val_loss: 0.4039)\n",
      "Saving best model (epoch 10, val_loss: 0.4037)\n",
      "Epoch: 00010 - 3.7s - loss: 0.3856 - acc: 0.8249 - val_loss: 0.4037 - val_acc: 0.8154\n",
      "Saving best model (epoch 12, val_acc: 0.8164)\n",
      "Saving best model (epoch 12, val_loss: 0.4021)\n",
      "Saving best model (epoch 14, val_acc: 0.8185)\n",
      "Saving best model (epoch 14, val_loss: 0.4003)\n",
      "Epoch: 00015 - 3.6s - loss: 0.3809 - acc: 0.8283 - val_loss: 0.4016 - val_acc: 0.8164\n",
      "Epoch: 00020 - 3.7s - loss: 0.3766 - acc: 0.8304 - val_loss: 0.4044 - val_acc: 0.8157\n",
      "Saving best model (epoch 25, val_acc: 0.8190)\n",
      "Epoch: 00025 - 3.5s - loss: 0.3735 - acc: 0.8322 - val_loss: 0.4065 - val_acc: 0.8190\n",
      "Epoch: 00030 - 3.6s - loss: 0.3704 - acc: 0.8331 - val_loss: 0.4053 - val_acc: 0.8165\n",
      "Epoch: 00035 - 3.7s - loss: 0.3670 - acc: 0.8346 - val_loss: 0.4104 - val_acc: 0.8167\n",
      "Epoch: 00040 - 3.8s - loss: 0.3651 - acc: 0.8358 - val_loss: 0.4142 - val_acc: 0.8129\n",
      "Epoch: 00045 - 3.6s - loss: 0.3622 - acc: 0.8372 - val_loss: 0.4151 - val_acc: 0.8165\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "# train model 1, get model parameters\n",
    "nn_architecture_1 = [\n",
    "    {\"input_dim\": 21, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "param_1 = train(tX_train_1, y_train_1, tX_val_1, y_val_1, nn_architecture_1, epochs, learning_rate, batch_size, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00000 - 0.7s - loss: 0.3991 - acc: 0.8197 - val_loss: 0.3768 - val_acc: 0.8395\n",
      "Epoch: 00001 - 0.7s - loss: 0.3658 - acc: 0.8402 - val_loss: 0.3673 - val_acc: 0.8403\n",
      "Saving best model (epoch 2, val_acc: 0.8464)\n",
      "Saving best model (epoch 2, val_loss: 0.3586)\n",
      "Saving best model (epoch 5, val_loss: 0.3578)\n",
      "Epoch: 00005 - 2.7s - loss: 0.3425 - acc: 0.8500 - val_loss: 0.3578 - val_acc: 0.8451\n",
      "Saving best model (epoch 6, val_loss: 0.3577)\n",
      "Saving best model (epoch 8, val_loss: 0.3576)\n",
      "Epoch: 00010 - 3.3s - loss: 0.3285 - acc: 0.8566 - val_loss: 0.3605 - val_acc: 0.8457\n",
      "Epoch: 00015 - 3.4s - loss: 0.3207 - acc: 0.8597 - val_loss: 0.3652 - val_acc: 0.8447\n",
      "Epoch: 00020 - 3.5s - loss: 0.3125 - acc: 0.8617 - val_loss: 0.3667 - val_acc: 0.8454\n",
      "Epoch: 00025 - 3.5s - loss: 0.3047 - acc: 0.8661 - val_loss: 0.3723 - val_acc: 0.8413\n",
      "Epoch: 00030 - 3.6s - loss: 0.2997 - acc: 0.8686 - val_loss: 0.3845 - val_acc: 0.8373\n",
      "Epoch: 00035 - 3.5s - loss: 0.2950 - acc: 0.8706 - val_loss: 0.3861 - val_acc: 0.8379\n",
      "Epoch: 00040 - 3.5s - loss: 0.2907 - acc: 0.8719 - val_loss: 0.3946 - val_acc: 0.8337\n",
      "Epoch: 00045 - 3.6s - loss: 0.2862 - acc: 0.8741 - val_loss: 0.3963 - val_acc: 0.8338\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "learning_rate = 0.005\n",
    "batch_size = 64\n",
    "\n",
    "# train model 2, get model parameters\n",
    "nn_architecture_2 = [\n",
    "    {\"input_dim\": 29, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 64, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "param_2 = train(tX_train_2, y_train_2, tX_val_2, y_val_2, nn_architecture_2, epochs, learning_rate, batch_size, True, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_0 = np.load('best_acc_0.npy', allow_pickle=True).item()\n",
    "param_1 = np.load('best_acc_1.npy', allow_pickle=True).item()\n",
    "param_2 = np.load('best_acc_2.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep neural network - jetnum = 0\n",
      "Training Loss: 0.3251 - Training Accuracy: 0.8556\n",
      "Validation Loss: 0.3419 - Validation Accuracy: 0.8491\n",
      "\n",
      " Deep neural network - jetnum = 1\n",
      "Training Loss: 0.3715 - Training Accuracy: 0.8325\n",
      "Validation Loss: 0.4065 - Validation Accuracy: 0.8190\n",
      "\n",
      " Deep neural network - jetnum = 2\n",
      "Training Loss: 0.3443 - Training Accuracy: 0.8492\n",
      "Validation Loss: 0.3586 - Validation Accuracy: 0.8464\n"
     ]
    }
   ],
   "source": [
    "# get final output of the model - group 0 \n",
    "y_train_hat, _ = full_forward_propagation(tX_train_0, param_0, nn_architecture_0)\n",
    "y_val_hat, _ = full_forward_propagation(tX_val_0, param_0, nn_architecture_0)\n",
    "\n",
    "# compute loss and accuracy\n",
    "train_loss = compute_nn_loss(y_train_hat, y_train_0)\n",
    "val_loss = compute_nn_loss(y_val_hat, y_val_0)\n",
    "\n",
    "train_acc = compute_nn_accuracy(y_train_hat, y_train_0)\n",
    "val_acc = compute_nn_accuracy(y_val_hat, y_val_0)\n",
    "\n",
    "print('Deep neural network - jetnum = 0')\n",
    "print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))\n",
    "\n",
    "# get final output of the model\n",
    "y_train_hat, _ = full_forward_propagation(tX_train_1, param_1, nn_architecture_1)\n",
    "y_val_hat, _ = full_forward_propagation(tX_val_1, param_1, nn_architecture_1)\n",
    "\n",
    "# compute loss and accuracy - group 1\n",
    "train_loss = compute_nn_loss(y_train_hat, y_train_1)\n",
    "val_loss = compute_nn_loss(y_val_hat, y_val_1)\n",
    "\n",
    "train_acc = compute_nn_accuracy(y_train_hat, y_train_1)\n",
    "val_acc = compute_nn_accuracy(y_val_hat, y_val_1)\n",
    "\n",
    "print('\\n Deep neural network - jetnum = 1')\n",
    "print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))\n",
    "\n",
    "# get final output of the model\n",
    "y_train_hat, _ = full_forward_propagation(tX_train_2, param_2, nn_architecture_2)\n",
    "y_val_hat, _ = full_forward_propagation(tX_val_2, param_2, nn_architecture_2)\n",
    "\n",
    "# compute loss and accuracy - group 2\n",
    "train_loss = compute_nn_loss(y_train_hat, y_train_2)\n",
    "val_loss = compute_nn_loss(y_val_hat, y_val_2)\n",
    "\n",
    "train_acc = compute_nn_accuracy(y_train_hat, y_train_2)\n",
    "val_acc = compute_nn_accuracy(y_val_hat, y_val_2)\n",
    "\n",
    "print('\\n Deep neural network - jetnum = 2')\n",
    "print(\"Training Loss: {:.4f} - Training Accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "print(\"Validation Loss: {:.4f} - Validation Accuracy: {:.4f}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of testing data\n",
    "y_test_hat_0, _ = full_forward_propagation(test_x_0.values.T, param_0, nn_architecture_0)\n",
    "y_test_pred_0 = np.where(np.squeeze(y_test_hat_0.T) > 0.5, 1, -1)\n",
    "y_test_hat_1, _ = full_forward_propagation(test_x_1.values.T, param_1, nn_architecture_1)\n",
    "y_test_pred_1 = np.where(np.squeeze(y_test_hat_1.T) > 0.5, 1, -1)\n",
    "y_test_hat_2, _ = full_forward_propagation(test_x_2.T, param_2, nn_architecture_2)\n",
    "y_test_pred_2 = np.where(np.squeeze(y_test_hat_2.T) > 0.5, 1, -1)\n",
    "\n",
    "# concatenate three predicted results\n",
    "y_test_pred_tmp = np.concatenate((y_test_pred_0, y_test_pred_1), axis = 0)\n",
    "y_test_pred = np.concatenate((y_test_pred_tmp, y_test_pred_2), axis = 0)\n",
    "\n",
    "# concatenate the indexs of three group\n",
    "ids = np.concatenate((id_0, id_1), axis = 0)\n",
    "ids = np.concatenate((ids, id_2), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'nn_w_pre.csv'\n",
    "create_csv_submission(ids, y_test_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
